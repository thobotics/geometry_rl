# @package _global_

# specify here default training configuration
defaults:
  - hydra: default
  - algorithm: trpl
  - collector: default
  - logger: default
  - env: default
  - simulator: default

  # enable color logging
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog
  
  # specify here what to override for default configurations, e.g.:
  - override env/transform: normalize_3d_and_clip
  - override algorithm/policy: legacy_gnn
  - override algorithm/policy/p2c_agent/data: ../../../p2c_agent/data/rope_tasks
  - override algorithm/policy/p2c_agent/model: ../../../p2c_agent/model/hetero_ponita_2
  - override algorithm/value: legacy_gnn
  - override algorithm/value/p2c_agent/data: ../../../p2c_agent/data/rope_tasks
  - override algorithm/value/p2c_agent/model: ../../../p2c_agent/model/deepsets
  - _self_



# global configurations
experiment_name: ${env.name}_${algorithm.name}

# path to original working directory (that `run.py` was executed from in command line)
# hydra hijacks working directory by changing it to the current log directory,
# so it's useful to have path to original work dir as a special variable
# read more here: https://hydra.cc/docs/next/tutorials/basic/running_your_app/working_directory
work_dir: ${hydra:runtime.cwd}

logger:
  checkpoint:
    save_interval: 5

# specific configuration
env:
  name: Isaac-Rope-Shaping-v0
  num_envs: 200
  warmup_steps: 10
  transform:
    - _target_: ironlib.torchrl.envs.ReshapeTransform
      in_keys: ["position_vectors", "velocity_vectors"]
      out_shape: [-1, 3]  # reshape to preserve geometry information

    - _target_: ironlib.torchrl.envs.NDVecNorm
      in_keys: ["position_vectors", "velocity_vectors"]
      out_keys: ["norm_position_vectors", "norm_velocity_vectors"]
      shapes: [3, 3]
      decay: 0.99999
      eps: 1e-2

    - _target_: torchrl.envs.VecNorm
      in_keys: ["scalars"]
      decay: 0.99999
      eps: 1e-2

    - _target_: torchrl.envs.FlattenObservation
      in_keys: ["norm_position_vectors", "norm_velocity_vectors", "position_vectors", "velocity_vectors"]
      first_dim: -2
      last_dim: -1

    - _target_: torchrl.envs.ClipTransform
      in_keys: ["scalars", "position_vectors", "velocity_vectors", "norm_position_vectors", "norm_velocity_vectors"]
      low: -50.0
      high: 50.0

    - _target_: torchrl.envs.RewardSum

    - _target_: torchrl.envs.StepCounter

collector:
  frames_per_batch: 40_000  # 200 envs * 100 steps
  total_frames: 10_000_000

latent_dim: &latent_dim 64  # ${algorithm.policy.hidden_sizes.0}

algorithm:
  policy:
    contextual_std: True
    post_fc: False  # NOTE: this is different from the original configuration
    hidden_sizes:
    - *latent_dim
    - *latent_dim  # 128  # *latent_dim
    in_features:
      - scalars
      - position_vectors
      - velocity_vectors
      - norm_position_vectors
      - norm_velocity_vectors
    share_action_dim: True
    p2c_agent:
      data:
        base_data:
          concat_input_vector: False # NOTE: this is different from the original configuration
          full_graph_obs: False
          dist_as_pos: True  #  False  # True
          training_noise: False
          training_noise_std: 0.01
          output_mask_key: grippers
        input_node_aux_dim: 3  # 3 vectors
      model:
        concat_global: False  # True
        num_ori: 16
        ponita_dim: 2
        # only_upper_hemisphere: True
  value:
    hidden_sizes:
      - *latent_dim
      - *latent_dim
    in_features:
      - scalars
      - position_vectors
      - velocity_vectors
      - norm_position_vectors
      - norm_velocity_vectors
    p2c_agent:
      data:
        base_data:
          full_graph_obs: True
          dist_as_pos: False
          training_noise: False
          output_mask_key: null
        input_node_aux_dim: 9
  objective:
    ppo_epochs: 5
    mini_batch_size: 200
    critic_coef: 0.5
    entropy_coef: 0.005
    anneal_clip_epsilon: False
    clip_grad_norm: True
  projection:
    trust_region_coeff: 1.0
    mean_bound: 0.05 # 0.1
    cov_bound: 0.0025 # 0.005
  optim:
    lr: 3e-4
    anneal_lr: False